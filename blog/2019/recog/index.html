<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>AR探索-camera动态图像识别</title>
  <meta name="description"
    content="  接着上节继续， 上节使用了一个特制的maker（印有方块的标记）。 现实生活中，这种标记都是不存在的，这里通过算法对现实世界的图像进行特征提取，生成特有的标记， 从而实现了从基于标记到AR转移到无标记的AR。可采用无标记AR的几个例子： 杂志封面，公司标志、玩具等。无标记AR计算量很大， 所以移动设备往往不能...">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2019/recog/">
  <link rel="alternate" type="application/rss+xml" title="Huailiang Blog"
    href="https://huailiang.github.io/feed.xml" />

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
        <!-- <li><a href="/feed.xml" target="_blank"><i class="icon icon-feed"></i></a></li> -->
        <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">AR探索-camera动态图像识别</h1>
      <p class="post-meta">Aug 8, 2019 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>接着上节继续， 上节使用了一个特制的maker（印有方块的标记）。 现实生活中，这种标记都是不存在的，这里通过算法对现实世界的图像进行特征提取，生成特有的标记， 从而实现了从基于标记到AR转移到无标记的AR。可采用无标记AR的几个例子： 杂志封面，公司标志、玩具等。无标记AR计算量很大， 所以移动设备往往不能确保流畅的FPS。</p>
</blockquote>

<p>在实现过程主要分为两步：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>1. 使用关联图片创建一个marker

2. 和新图片匹配
</code></pre>
</div>

<h2 id="生成marker">生成marker</h2>

<p>首先，使用选择手机相册里的图片，根据UIImagePickerController的回调内容，把数据传递给OpenGL. Opengl在screen右下角画出选择的image， 就像之前在glfw里显示shadowmap一样。</p>

<p>获取UIImagePickerController的回调， 你需要在代码注册protocol，类似这样:</p>

<figure class="highlight"><pre><code class="language-oc" data-lang="oc">@interface AlbumSource() &lt;UINavigationControllerDelegate, UIImagePickerControllerDelegate&gt;
{}

-(void)imagePickerControllerDidCancel:(UIImagePickerController *)picker;

-(void)imagePickerController:(UIImagePickerController *)picker
 didFinishPickingMediaWithInfo:(NSDictionary&lt;NSString *,id&gt; *)info;

@end</code></pre></figure>

<p>使用OpenGL画选择的图片就更简单了。 render 一张texture， 绑定一张mesh， mesh的uv放在右下角：</p>

<div class="language-cpp highlighter-rouge"><pre class="highlight"><code><span class="c1">// generate a texture
</span><span class="n">glGenTextures</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">texID</span><span class="p">);</span>
<span class="n">glBindTexture</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">texID</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_MIN_FILTER</span><span class="p">,</span> <span class="n">GL_LINEAR</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_MAG_FILTER</span><span class="p">,</span> <span class="n">GL_LINEAR</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_WRAP_S</span><span class="p">,</span> <span class="n">GL_CLAMP_TO_EDGE</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_WRAP_T</span><span class="p">,</span> <span class="n">GL_CLAMP_TO_EDGE</span><span class="p">);</span>

<span class="c1">// render the texture
</span><span class="n">glBindTexture</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">texID</span><span class="p">);</span>
<span class="n">glTexImage2D</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">GL_RGBA</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">GL_RGBA</span><span class="p">,</span> <span class="n">GL_UNSIGNED_BYTE</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>
</code></pre>
</div>

<p>同时传递给cv, 生成关键点和descriptor, 后面使用匹配算法会用到这两个变量：</p>

<div class="language-cpp highlighter-rouge"><pre class="highlight"><code><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">image</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">CV_8UC4</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>
<span class="n">cornerDetector</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">ORB</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="mi">750</span><span class="p">);</span>
<span class="n">cornerDetector</span><span class="o">-&gt;</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> 
  <span class="n">cv</span><span class="o">::</span><span class="n">noArray</span><span class="p">(),</span> 
  <span class="n">referenceKeypoints</span><span class="p">,</span> 
  <span class="n">referenceDescriptors</span><span class="p">);</span>
</code></pre>
</div>

<p>图像的特征点可以简单的理解为图像中比较显著显著的点，如轮廓点，较暗区域中的亮点，较亮区域中的暗点等。ORB采用FAST（features from accelerated segment test）算法来检测特征点。这个定义基于特征点周围的图像灰度值，检测候选特征点周围一圈的像素值，如果候选点周围领域内有足够多的像素点与该候选点的灰度值差别够大，则认为该候选点为一个特征点。</p>

<script type="math/tex; mode=display">N =  \sum_{x∀(circle(p))}| I_x - I_p | > \xi_d</script>

<p>为了获得更快的结果，还采用了额外的加速办法。如果测试了候选点周围每隔90度角的4个点，应该至少有3个和候选点的灰度值差足够大，否则则不用再计算其他点，直接认为该候选点不是特征点。候选点周围的圆的选取半径是一个很重要的参数，这里为了简单高效，采用半径为3，共有16个周边像素需要比较。为了提高比较的效率，通常只使用N个周边像素来比较，也就是大家经常说的FAST-N.</p>

<center><img src="/img/post-vr/vr1.png" /></center>
<p><br /><br /></p>

<h4 id="brief特征描述子">BRIEF特征描述子</h4>

<p>得到特征点后我们需要以某种方式描述这些特征点的属性。这些属性的输出我们称之为该特征点的描述子（Feature DescritorS）.ORB采用BRIEF算法来计算一个特征点的描述子。<br />
BRIEF算法的核心思想是在关键点P的周围以一定模式选取N个点对，把这N个点对的比较结果组合起来作为描述子。</p>

<center><img src="/img/post-vr/vr9.png" /></center>
<p><br /><br /></p>

<p>步骤：<br />
1.以关键点P为圆心，以d为半径做圆O。<br />
2.在圆O内某一模式选取N个点对。这里为方便说明，N=4，实际应用中N可以取512.<br />
假设当前选取的4个点对如上图所示分别标记为：</p>

<p>算法步骤如下：</p>

<p>1.以关键点P为圆心，以d为半径做圆O。</p>

<p>2.在圆O内某一模式选取N个点对。这里为方便说明，N=4，实际应用中N可以取512. 假设当前选取的4个点对如上图所示分别标记为：</p>

<script type="math/tex; mode=display">P_1(A,B)、 P_2(A,B)、 P_3(A,B)、 P_4(A,B)</script>

<p>3.定义操作 <script type="math/tex">\tau</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\tau(p(A,B))=
\begin{cases}
0& {I_A<I_B}\\
1& {I_A>I_B}
\end{cases} %]]></script>

<p>其中<script type="math/tex">I_A</script> 表示点A的灰度</p>

<p>4.分别对已选取的点对进行T操作，将得到的结果进行组合。 假如：</p>

<script type="math/tex; mode=display">\tau(p_1(A,B)) = 1 \\
\tau(p_2(A,B)) = 0 \\
\tau(p_3(A,B)) = 1 \\
\tau(p_4(A,B)) = 1 \\</script>

<p>则最终的描述子为：1011</p>

<h4 id="理想的特征点描述子应该具备的属性">理想的特征点描述子应该具备的属性：</h4>

<p>在现实生活中，我们从不同的距离，不同的方向、角度，不同的光照条件下观察一个物体时，物体的大小，形状，明暗都会有所不同。但我们的大脑依然可以判断它是同一件物体。理想的特征描述子应该具备这些性质。即，在大小、方向、明暗不同的图像中，同一特征点应具有足够相似的描述子，称之为描述子的可复现性。</p>

<p>当以某种理想的方式分别计算描述子时，应该得出同样的结果。即描述子应该对光照（亮度）不敏感，具备尺度一致性（大小 ），旋转一致性（角度）等。</p>

<p>ORB并没有解决尺度一致性问题，在OpenCV的ORB实现中采用了图像金字塔来改善这方面的性能。ORB主要解决BRIEF描述子不具备旋转不变性的问题。</p>

<p>在当前关键点P周围以一定模式选取N个点对，组合这N个点对的T操作的结果就为最终的描述子。当我们选取点对的时候，是以当前关键点为原点，以水平方向为X轴，以垂直方向为Y轴建立坐标系。当图片发生旋转时，坐标系不变，同样的取点模式取出来的点却不一样，计算得到的描述子也不一样，这是不符合我们要求的。因此我们需要重新建立坐标系，使新的坐标系可以跟随图片的旋转而旋转。这样我们以相同的取点模式取出来的点将具有一致性。</p>

<p>ORB在计算BRIEF描述子时建立的坐标系是以关键点为圆心，以关键点和取点区域的形心的连线为X轴建立2维坐标系。</p>

<center><img src="/img/post-vr/vr10.png" /></center>
<p><br /><br /></p>

<p>我们知道圆心是固定的而且随着物体的旋转而旋转。当我们以PQ作为坐标轴时，在不同的旋转角度下，我们以同一取点模式取出来的点是一致的。这就解决了旋转一致性的问题。</p>

<h2 id="匹配图片">匹配图片</h2>

<h4 id="利用brief特征进行配准">利用BRIEF特征进行配准</h4>

<p>汉明距离：</p>

<p>汉明距离是以理查德•卫斯里•汉明的名字命名的。在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：<br />
1011101 与 1001001 之间的汉明距离是 2。<br />
2143896 与 2233796 之间的汉明距离是 3。<br />
“toned” 与 “roses” 之间的汉明距离是 3。<br />
给予两个任何的字码，10001001和10110001，即可决定有多少个相对位是不一样的。在此例中，有三个位不同。要决定有多少个位不同，只需将xor运算加诸于两个字码就可以，并在结果中计算有多个为1的位。例如：<br />
10001001<br />
Xor 10110001<br />
00111000<br />
两个字码中不同位值的数目称为汉明距离(Hamming distance) 。</p>

<p>特征点的匹配<br />
ORB算法最大的特点就是计算速度快 。 这首先得益于使用FAST检测特征点，FAST的检测速度正如它的名字一样是出了名的快。再次是使用BRIEF算法计算描述子，该描述子特有的2进制串的表现形式不仅节约了存储空间，而且大大缩短了匹配的时间。<br />
例如特征点A、B的描述子如下。<br />
A：10101011<br />
B：10101010<br />
我们设定一个阈值，比如80%。当A和B的描述子的相似度大于90%时，我们判断A,B是相同的特征点，即这2个点匹配成功。在这个例子中A,B只有最后一位不同，相似度为87.5%，大于80%。则A和B是匹配的。<br />
我们将A和B进行异或操作就可以轻松计算出A和B的相似度。而异或操作可以借组硬件完成，具有很高的效率，加快了匹配的速度。</p>

<div class="language-cpp highlighter-rouge"><pre class="highlight"><code>
<span class="cp">#define NN_MATCH_RATIO              0.8f
#define MIN_INLIER_COUNT            32
</span><span class="n">matcher</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">DescriptorMatcher</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="s">"BruteForce-Hamming"</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">matchcnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cv</span><span class="o">::</span><span class="n">DMatch</span><span class="o">&gt;&gt;</span> <span class="n">descriptorMatches</span><span class="p">;</span>
<span class="n">matcher</span><span class="o">-&gt;</span><span class="n">knnMatch</span><span class="p">(</span><span class="n">referenceDescriptors</span><span class="p">,</span> <span class="n">descriptor</span><span class="p">,</span> <span class="n">descriptorMatches</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">descriptorMatches</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">descriptorMatches</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">distance</span> <span class="o">&lt;</span> 
        <span class="n">NN_MATCH_RATIO</span> <span class="o">*</span> <span class="n">descriptorMatches</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">distance</span><span class="p">)</span> <span class="n">matchcnt</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">if</span> <span class="p">(</span><span class="n">matchcnt</span> <span class="o">&gt;=</span> <span class="n">MIN_INLIER_COUNT</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">state</span><span class="o">-&gt;</span><span class="n">setText</span><span class="p">(</span><span class="s">" key points matched "</span><span class="o">+</span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">matchcnt</span><span class="p">));</span>
    <span class="n">captrue</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
</div>

<p>上述代码设定的阈值即80%， 当匹配的特征点个数超过32个的时候， 我们即认为匹配到了之前在相册选中的图片。在引擎里点击pick按钮会选择一张相册里的图，然后不断匹配camera的图像，知道检测到跟旋转的图片匹配的画面，画面定格。再次点击pick，会重新匹配新选组的图片。</p>

<p>运行效果如下:</p>

<center><img src="/img/post-vr/vr11.jpg" /></center>
<p><br /><br /></p>

<p>参考:</p>

<p>[1] <a href="https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin">KNN 邻近算法</a><br />
[2] <a href="http://www.anandmuralidhar.com/blog/android/simple-ar/">Image Match</a><br />
[3] <a href="https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin">iOS内置的api 获取camera内置参数</a><br />
[4] <a href="https://mp.weixin.qq.com/s/S4b1OGjRWX1kktefyHAo8A">OpenCV中ORB特征提取与匹配</a><br />
[5] <a href="https://blog.csdn.net/weixin_41284198/article/details/81203577">OpenCV中ORB的API解释</a><br />
[6] <a href="http://cvlabwww.epfl.ch/%7Elepetit/papers/calonder_eccv10.pdf">百科中关于 汉明距离的介绍</a></p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-prev post-nav-item">
    
    <a href="/blog/2019/reverse/"> 破解《航海王-燃烧之血》</a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2019/vr/"> AR探索-固定标记识别</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2019 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>