<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>AR探索-固定标记识别</title>
  <meta name="description"
    content="  程序通过OpenCV实现对Marker的识别和定位，然后通过OpenGL将虚拟物体叠加到摄像头图像下，实现增强现实。这里以参考《深入理解OpenCV》这本书第二章节的例子，实现基于标记的的虚拟现实实现。看到网上很多的例子都是基于固定渲染管线来画虚拟物体的， 而且很多地方都经不起推敲。 本例使用现代OpenGL...">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2019/vr/">
  <link rel="alternate" type="application/rss+xml" title="Huailiang Blog"
    href="https://huailiang.github.io/feed.xml" />

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
        <!-- <li><a href="/feed.xml" target="_blank"><i class="icon icon-feed"></i></a></li> -->
        <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">AR探索-固定标记识别</h1>
      <p class="post-meta">Jul 18, 2019 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>程序通过OpenCV实现对Marker的识别和定位，然后通过OpenGL将虚拟物体叠加到摄像头图像下，实现增强现实。这里以参考《<a href="https://item.jd.com/32565591095.html">深入理解OpenCV</a>》这本书第二章节的例子，实现基于标记的的虚拟现实实现。看到网上很多的例子都是基于固定渲染管线来画虚拟物体的， 而且很多地方都经不起推敲。 本例使用现代OpenGL实现的整个绘制过程。</p>
</blockquote>

<h2 id="访问相机">访问相机</h2>

<p>增强现实应用必须包括视频捕获和AR可视化这两个主要过程。视频捕获阶段包括从设备接收视频帧，执行必要的色彩转换，并且将其发送给图像处理流程。对AR应用来讲，单帧处理的时间很关键，因此，帧的获取应尽可能高效。为了达到高性能，最好的办法是直接从摄像机读取帧。从iOS4开始支持这种方式。AVFoundation框架有现成的API函数来直接读取内存中的图像缓冲区。</p>

<p>AVCaptureDevice和AVCaptureVideoDataOutput允许用户配置、捕获以及指定未处理视频帧，这些帧都是32bpp BGRA格式（bpp是bit per pixel的缩写）。也可设置输出帧的分辨率。但这样做会影响整体性能，因为较大的帧会花费更多的处理时间并需要更大的内存空间。</p>

<p>通过AVFoundation API来获取高性能视频有一个好的选择。它提供了一个更快、更简洁的方法来直接从摄像机缓冲区中获取帧。但首先需了解下图关于iOS的视频获取流程：</p>

<p><img src="/img/post-vr/vr5.jpg" alt="" /></p>

<p>AVCaptureMovieFileOutput接口用于将视频写到文件中，AVCaptureStillImageOutput接口用于生成静态图像，AVCaptureVideoPreviewLayer接口用于在屏幕上进行视频预览。本项目将用到AVCaptureVideoDataOutput接口，因为它可直接访问视频数据。</p>

<pre><code class="language-mm">AVCaptureVideoDataOutput *captureOutput = [[AVCaptureVideoDataOutput alloc] init];

captureOutput.alwaysDiscardsLateVideoFrames = YES;

// 在这里注册输出
[self.captureSession addOutput:captureOutput];

#pragma mark AVCaptureSession delegate
- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection
{
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    CVPixelBufferLockBaseAddress(imageBuffer,0);
    
    /*Get information about the image*/
    uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer);
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    size_t stride = CVPixelBufferGetBytesPerRow(imageBuffer);
    
    BGRAVideoFrame frame = {width, height, stride, baseAddress};
    [delegate frameReady:frame];  //派发帧
    
	/*We unlock the  image buffer*/
	CVPixelBufferUnlockBaseAddress(imageBuffer,0);
} 
</code></pre>

<h2 id="绘制背景">绘制背景</h2>

<p>由于是AR项目， 所有背景输出的就是camera捕获的内容了。 这里获取到的frame都是上面代码传过来的BGRA格式的图像， 我们在OpenGL创建一个GL_TEXTURE_2D， 把frmae的内容绑定到对应的texture即可。对应的格式设置如下， 此时因为全屏输出， 也没有必要设置mipmap.</p>

<div class="language-cpp highlighter-rouge"><pre class="highlight"><code><span class="n">glGenTextures</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">m_backgroundTextureId</span><span class="p">);</span>
<span class="n">glBindTexture</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">m_backgroundTextureId</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_MIN_FILTER</span><span class="p">,</span> <span class="n">GL_LINEAR</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_MAG_FILTER</span><span class="p">,</span> <span class="n">GL_LINEAR</span><span class="p">);</span>
<span class="c1">// This is necessary for non-power-of-two textures
</span><span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_WRAP_S</span><span class="p">,</span> <span class="n">GL_CLAMP_TO_EDGE</span><span class="p">);</span>
<span class="n">glTexParameteri</span><span class="p">(</span><span class="n">GL_TEXTURE_2D</span><span class="p">,</span> <span class="n">GL_TEXTURE_WRAP_T</span><span class="p">,</span> <span class="n">GL_CLAMP_TO_EDGE</span><span class="p">);</span>
</code></pre>
</div>

<p>创建一个quad的mesh， 对应的uv区间是【0，1】。 在vert shdader里，直接输出到屏幕，也不需要多余的变换，例如投影之类的。如果输出的图像于现实是相反的，你可以在frag shader里uv采样的时候直接翻转下uv即可。</p>

<div class="language-glsl highlighter-rouge"><pre class="highlight"><code><span class="c1">// vert shader
</span>
<span class="cp">#version 330 core
</span><span class="k">layout</span> <span class="p">(</span><span class="n">location</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="k">in</span> <span class="kt">vec3</span> <span class="n">aPos</span><span class="p">;</span>
<span class="k">layout</span> <span class="p">(</span><span class="n">location</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="k">in</span> <span class="kt">vec2</span> <span class="n">aTexCoords</span><span class="p">;</span>
<span class="k">out</span> <span class="kt">vec2</span> <span class="n">TexCoords</span><span class="p">;</span>

<span class="nb">gl_Position</span> <span class="o">=</span> <span class="kt">vec4</span><span class="p">(</span><span class="n">aPos</span><span class="p">,</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// frag shader
</span>
<span class="cp">#version 330 core
</span>
<span class="k">out</span> <span class="kt">vec4</span> <span class="n">FragColor</span><span class="p">;</span>
<span class="k">in</span> <span class="kt">vec2</span> <span class="n">TexCoords</span><span class="p">;</span>

<span class="cp">#ifdef _FLIP_Y_
</span>    <span class="kt">vec4</span> <span class="n">color</span> <span class="o">=</span> <span class="n">texture</span><span class="p">(</span><span class="n">texture1</span><span class="p">,</span> <span class="kt">vec2</span><span class="p">(</span><span class="n">TexCoords</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span> <span class="o">-</span> <span class="n">TexCoords</span><span class="p">.</span><span class="n">y</span><span class="p">));</span>
<span class="cp">#end
</span><span class="n">FragColor</span> <span class="o">=</span> <span class="n">color</span><span class="p">;</span>
</code></pre>
</div>

<h2 id="相机姿势估计">相机姿势估计</h2>

<p>关于什么是相机的内参和外参就不详细介绍了， 具体可以<a href="http://zhixinliu.com/2016/11/15/2016-11-15-camera-intrinsic/">参考这篇文章</a>。</p>

<center>
    <img src="/img/post-vr/vr2.png" height="240" />
</center>
<blockquote>
  <p><br /><br /></p>
</blockquote>

<p>计算摄像机的位置，首先需要对摄像机进行标定，标定是确定摄像机内参矩阵K的过程，一般用棋盘进行标定，这已经是一个很成熟的方法了，在这就不细说了。得到相机的内参矩阵K后，就可以使用solvePnP方法求取摄像机关于某个Marker的位置（这里理解成相机的位置和旋转更确切些， 以虚拟物品当做世界坐标原点，即虚拟物品的模型空间等于世界空间，通过得来的转换矩阵变换到view空间）。摄像机成像使用小孔模型，如下：</p>

<p><img src="/img/post-vr/vr7.jpg" alt="" /></p>

<script type="math/tex; mode=display">x = K[R|T]X</script>

<p>其中，X是空间某点的坐标（相对于世界坐标系），R和T是摄像机外参矩阵，用于将某点的世界坐标变换为摄像机坐标，K是摄像机内参，用于将摄像机坐标中的某点投影的像平面上，x即为投影后的像素坐标。</p>

<p>相机的内参K 表达式如下:</p>

<script type="math/tex; mode=display">% <![CDATA[
K =
\left[
 \begin{matrix}
   f_x & s & x_0 \\
   0 & f_y & y_0  \\
   0 & 0 & 1 
  \end{matrix} 
\right] %]]></script>

<p>其中f_x代表x-axi方向上的焦距， f_y代表着y-axi方向上的矩阵， 一般来说f_x = f_y = f ， 即相机的焦距，在iPhone或者iOS设备上，这个值大约等于640。 x_0和y_0有的地方也叫u0和v0,表示图像的半宽和半高。这里的参数主要用在投影过程中， 他们都是投影矩阵的参数，下面会有详细的介绍。</p>

<h2 id="opencv的相机坐标系转换到opengl的相机坐标系">opencv的相机坐标系转换到opengl的相机坐标系</h2>

<p>opencv的相机坐标系参考文章：<a href="https://www.2cto.com/kf/201607/530281.html">相机模型与标定（二）–三大坐标系及关系</a></p>

<p>opengl的相机坐标系参考文章：<a href="https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/">OpenGL.坐标系统的介绍与坐标变换的实现</a></p>

<center>
    <img src="/img/post-vr/vr6.png" height="240" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/img/post-vr/vr3.png" height="240" />
</center>

<p>两个系统下的相机坐标系都是右手系的，x轴都是指向右，只是y轴和z轴反了。（上左图就是opencv的相机坐标系，上右图是opengl的）因此，只需要把物体在opencv的相机坐标系下的坐标绕x轴旋转180度，就可以转换为opengl的相机坐标系下的坐标了。</p>

<p>具体实现可以让相机外参左乘一个翻转矩阵：</p>

<script type="math/tex; mode=display">% <![CDATA[
reverseYZ =
\left[
 \begin{matrix}
   1 & 0 & 0 & 0 \\
   0 & -1 & 0 & 0 \\
   0 & 0 & -1 & 0 \\
   0 & 0 & 0 & 1 \\
  \end{matrix} 
\right] %]]></script>

<p>获取使用一个3x3的矩阵同时作用于transform(vec3) 和旋转(mat3). 我看网上有类似的计算，也是对的。</p>

<script type="math/tex; mode=display">% <![CDATA[
M =
\left[
 \begin{matrix}
   1 & 0 & 0  \\
   0 & -1 & 0  \\
   0 & 0 & -1 
  \end{matrix} 
\right] %]]></script>

<p>在<a href="http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/">ksimek的博客</a>里， 看到一个思路，并没有翻转y轴和z轴。 而是添加了一个到NDC的变换，可能也能实现同样的效果。 但是文章里很多公式都是错的，这里我也没有仔细推。耐心的读者可以认真地按照作者说的思路，详细推导一遍。</p>

<p>读者可以看到很多<a href="https://blog.csdn.net/yanglusheng/article/details/52268234">地方</a>说是需要绕x轴旋转180, 也可以推导出上述的公式。 但原因绝不是因为view space到project space的变化的，也不是因为所谓的OpenGL为了进行Clipping，其投影矩阵需要将点投影到NDC空间中。 因为这些变化都体现在了投影矩阵里了， 投影矩阵会切换右手坐标系到左手坐标系，具体的推导过程参考<a href="http://www.songho.ca/opengl/gl_projectionmatrix.html#ortho">这里</a>。</p>

<script type="math/tex; mode=display">% <![CDATA[
P =
\left[
 \begin{matrix}
   \frac{2f_x}{w} & 0 & 0 & 0 \\
   0 & \frac{2f_y}{h} & 0 & 0 \\
   0 & 0 & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\
   0 & 0 & -1 & 0 \\
  \end{matrix} 
\right] %]]></script>

<p>现在再来考虑OpenGL投影椎体不对称的情况，这种情况下，PROJECTION矩阵的形式为：</p>

<script type="math/tex; mode=display">% <![CDATA[
P =
\left[
 \begin{matrix}
   \frac{2f_x}{w} & 0 & c  & 0 \\
   0 & \frac{2f_y}{h} & d & 0 \\
   0 & 0 & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\
   0 & 0 & -1 & 0 \\
  \end{matrix} 
\right] %]]></script>

<p>其中c,d 如下:</p>

<script type="math/tex; mode=display">c= \frac{l+r}{w} = \frac{w-2c_x}{w} = 1 - \frac{2c_x}{w}</script>

<script type="math/tex; mode=display">d  = \frac{b+t}{h} = \frac{2c_y-h}{h} = \frac{2c_y}{h} - 1</script>

<p>关于l+r和b+t是怎么计算的，可以参考下图：</p>

<center>
    <img src="/img/post-vr/vr8.jpg" height="280" />
</center>
<blockquote>
  <p><br /><br /></p>
</blockquote>

<script type="math/tex; mode=display">% <![CDATA[
P =
\left[
 \begin{matrix}
   \frac{2f_x}{w} & 0 & 1-\frac{2c_x}{w} & 0 \\
   0 & \frac{2f_y}{h} & \frac{2c_y}{h} - 1 & 0 \\
   0 & 0 & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\
   0 & 0 & -1 & 0 \\
  \end{matrix} 
\right] %]]></script>

<p>代码部分如下， 根据相机内参构建投影矩阵并建立opencv到opengl的y轴-z轴翻转矩阵（绕x轴旋转180度）</p>

<div class="language-cpp highlighter-rouge"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">InitialVR</span><span class="p">(</span><span class="kt">float</span> <span class="n">width</span><span class="p">,</span> <span class="kt">float</span> <span class="n">height</span><span class="p">,</span><span class="k">const</span> <span class="n">Matrix33</span><span class="o">&amp;</span> <span class="n">intrinsic</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">glm</span><span class="o">::</span><span class="n">mat4</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="kt">float</span> <span class="n">fx</span> <span class="o">=</span> <span class="n">intrinsic</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span> <span class="o">-</span><span class="mf">0.01</span><span class="n">f</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="o">-</span><span class="mf">100.0</span><span class="p">;</span>
    <span class="n">proj</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">fx</span> <span class="o">/</span> <span class="n">width</span><span class="p">;</span>
    <span class="n">proj</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">fx</span> <span class="o">/</span> <span class="n">height</span><span class="p">;</span>
    <span class="n">proj</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">f</span><span class="p">);</span>
    <span class="n">proj</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">;</span>
    <span class="n">proj</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">f</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">(</span> <span class="n">f</span> <span class="o">-</span> <span class="n">n</span><span class="p">);</span>
    
    <span class="n">reverse</span> <span class="o">=</span> <span class="n">glm</span><span class="o">::</span><span class="n">mat4</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">reverse</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="n">reverse</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
</div>

<p>根据MarkerDetector识别的camera外部参数, 传递给shader.</p>

<div class="language-c++ highlighter-rouge"><pre class="highlight"><code><span class="n">glm</span><span class="o">::</span><span class="n">mat4</span> <span class="n">view</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">getMat44</span><span class="p">();</span> <span class="c1">//camera's position &amp; rotation
</span><span class="n">view</span> <span class="o">=</span> <span class="n">reverse</span> <span class="o">*</span> <span class="n">view</span><span class="p">;</span>
<span class="n">vrShader</span><span class="o">-&gt;</span><span class="n">use</span><span class="p">();</span>
<span class="n">vrShader</span><span class="o">-&gt;</span><span class="n">setMat4</span><span class="p">(</span><span class="s">"view"</span><span class="p">,</span>  <span class="n">view</span><span class="p">);</span>
<span class="n">vrShader</span><span class="o">-&gt;</span><span class="n">setMat4</span><span class="p">(</span><span class="s">"proj"</span><span class="p">,</span> <span class="n">proj</span><span class="p">);</span>
<span class="n">glBindVertexArray</span><span class="p">(</span><span class="n">vrVao</span><span class="p">);</span>
<span class="n">glDrawArrays</span><span class="p">(</span><span class="n">DRAW_MODE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">36</span><span class="p">);</span>
<span class="n">glBindVertexArray</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</code></pre>
</div>

<p>shader里直接变换到屏幕上的图像就可以了, 在glsl里实现如下：</p>

<div class="language-glsl highlighter-rouge"><pre class="highlight"><code><span class="k">uniform</span> <span class="kt">mat4</span> <span class="n">view</span><span class="p">;</span>
<span class="k">uniform</span> <span class="kt">mat4</span> <span class="n">proj</span><span class="p">;</span>

<span class="k">out</span> <span class="kt">vec2</span> <span class="n">TexCoords</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="kt">vec3</span> <span class="n">pos</span> <span class="o">=</span> <span class="kt">vec3</span><span class="p">(</span><span class="n">aPos</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">aPos</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">aPos</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">);</span>
    <span class="nb">gl_Position</span> <span class="o">=</span> <span class="n">proj</span> <span class="o">*</span> <span class="n">view</span> <span class="o">*</span>  <span class="kt">vec4</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre>
</div>

<p>最后展示下在引擎里运行在ipad上的效果:</p>

<p><img src="/img/post-vr/vr4.jpg" alt="" /></p>

<p>参考:</p>

<p>[1] <a href="http://zhixinliu.com/2016/11/15/2016-11-15-camera-intrinsic/">Camera Intrinsic &amp; Extrinsic矩阵</a><br />
[2] <a href="http://www.songho.ca/opengl/gl_projectionmatrix.html#ortho">OpenGL Projection Matrix</a><br />
[3] <a href="https://developer.apple.com/documentation/avfoundation/avcameracalibrationdata">iOS内置的api 获取camera内置参数</a><br />
[4] <a href="https://blog.csdn.net/yanglusheng/article/details/52268234">OpenGL与OpenCV实现增强现实</a><br />
[5] <a href="https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/">OpenGL坐标系统</a><br />
[6] <a href="http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/">Calibrated Cameras in OpenGL without glFrustum</a></p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-prev post-nav-item">
    
    <a href="/blog/2019/recog/"> AR探索-camera动态图像识别</a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2019/harmonics/"> 球谐光照</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2019 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>