<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>TensorFlow-神经网络</title>
  <meta name="description"
    content="  人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2018/tensorflow/">
  <link rel="alternate" type="application/rss+xml" title="Huailiang Blog"
    href="https://huailiang.github.io/feed.xml" />

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
        <!-- <li><a href="/feed.xml" target="_blank"><i class="icon icon-feed"></i></a></li> -->
        <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">TensorFlow-神经网络</h1>
      <p class="post-meta">Mar 9, 2018 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。</p>
</blockquote>

<p>上来先说一个例子。城里正在举办一年一度的游戏动漫展览，小明拿不定主意，周末要不要去参观。</p>

<p>他决定考虑三个因素。</p>
<ul>
  <li>天气：周末是否晴天？</li>
  <li>同伴：能否找到人一起去？</li>
  <li>价格：门票是否可承受？</li>
</ul>

<p><img src="/img/post-tf/tf20.jpg" alt="" /></p>

<p>现实中，各种因素很少具有同等重要性：某些因素是决定性因素，另一些因素是次要因素。因此，可以给这些因素指定权重（W），代表它们不同的重要性。</p>

<p>权重（W）和阈值(b)<br />
现实中，各种因素很少具有同等重要性：某些因素是决定性因素，另一些因素是次要因素。因此，可以给这些因素指定权重（weight），代表它们不同的重要性。</p>

<ul>
  <li>天气：权重为8</li>
  <li>同伴：权重为4</li>
  <li>价格：权重为4</li>
</ul>

<p>这时，还需要指定一个阈值（threshold）。如果总和大于阈值，感知器输出1，否则输出0。假定阈值为8，那么 12 &gt; 8，小明决定去参观。阈值的高低代表了意愿的强烈，阈值越低就表示越想去，越高就越不想去.</p>

<h3 id="决策模型">决策模型</h3>

<p>单个的感知器构成了一个简单的决策模型，已经可以拿来用了。真实世界中，实际的决策模型则要复杂得多，是由多个感知器组成的多层网络。</p>

<p>外部因素 x1、x2、x3 写成矢量 &lt;x1, x2, x3&gt;，简写为 x</p>

<p>权重 w1、w2、w3 也写成矢量 (w1, w2, w3)，简写为 w</p>

<p>定义运算 w⋅x = ∑ wx，即 w 和 x 的点运算，等于因素与权重的乘积之和<br />
定义 b 等于负的阈值 b = -threshold</p>

<p>感知器模型就变成了下面这样:</p>

<p><img src="/img/post-tf/tf21.png" alt="" /></p>

<h3 id="神经网络的运作过程">神经网络的运作过程</h3>

<p>一个神经网络的搭建，需要满足三个条件。</p>

<ul>
  <li>输入和输出</li>
  <li>权重（w）和阈值（b）</li>
  <li>多层感知器的结构</li>
</ul>

<p><img src="/img/post-tf/timg.jpeg" alt="" /></p>

<p>其中，最困难的部分就是确定权重（w）和阈值（b）。目前为止，这两个值都是主观给出的，但现实中很难估计它们的值，必需有一种方法，可以找出答案。<br />
这种方法就是试错法。其他参数都不变，w（或b）的微小变动，记作Δw（或Δb），然后观察输出有什么变化。不断重复这个过程，直至得到对应最精确输出的那组w和b，就是我们要的值。这个过程称为模型的训练。</p>

<h3 id="tensorflow">Tensorflow</h3>

<p>关于 TensorFlow 的基础知识的学习，读者可以参考上一节：<a href="https://huailiang.github.io/2018/03/08/tensor/">https://huailiang.github.io/2018/03/08/tensor/</a></p>

<p><b>求权重和阈值</b></p>

<p>当然 Google 给了我们一套 api，通过大量的计算，能够得出正确的值。</p>

<ul>
  <li>
    <p>梯度下降算法</p>

    <p>梯度下降算法是用的最普遍的优化算法，不过梯度下降算法需要用到全部的样本，训练速度比较慢，但是迭代到一定次数最终能够找到最优解。</p>

    <p>tf.train.GradientDescentOptimizer（0.01）</p>

    <p>这个类是实现梯度下降算法的优化器，参数learning_rate是要使用的学习率 。详细点击官方<a href="https://tensorflow.google.cn/api_docs/python/tf/train/GradientDescentOptimizer">API</a>。</p>
  </li>
  <li>
    <p>选择 optimizer 使 loss 达到最小</p>

    <p>optimizer.minimize(loss)</p>
  </li>
</ul>

<p>我们定义 train 训练模型使损失降到最低，如此反复的训练，求得 权重（w）和阈值(b)的最优解。</p>

<p>就不过多解释了，代码贴出来了：</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># -*- coding:utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">board</span> <span class="kn">import</span> <span class="n">Board</span>


<span class="c"># 变量</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.1</span><span class="p">],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c"># placeholder</span>
<span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">6.1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">11</span><span class="p">]</span>
<span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">x_train</span><span class="p">,</span><span class="n">y</span><span class="p">:</span><span class="n">y_train</span><span class="p">}</span>

<span class="n">linear_mode</span><span class="o">=</span><span class="n">W</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span>
<span class="n">square_details</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">linear_mode</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">square_details</span><span class="p">)</span>

<span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">"loss"</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
<span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
   <span class="n">summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span> <span class="s">"output/"</span><span class="p">,</span><span class="n">sess</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
   <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
   <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

   <span class="k">print</span><span class="p">(</span><span class="s">"real vale: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">linear_mode</span><span class="p">,</span><span class="n">feed_dict</span><span class="p">)))</span>
   <span class="k">print</span><span class="p">(</span><span class="s">"curr loss: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">feed_dict</span><span class="p">)))</span>

   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
       <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">feed_dict</span><span class="p">)</span>
       <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
           <span class="n">cc</span><span class="p">,</span><span class="n">res</span><span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span><span class="n">merged</span><span class="p">],</span><span class="n">feed_dict</span><span class="p">)</span>
           <span class="n">summary_writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
           <span class="k">print</span> <span class="s">"train step(</span><span class="si">%</span><span class="s">s) loss:</span><span class="si">%</span><span class="s">s"</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">cc</span><span class="p">)</span>

   <span class="c"># Evaluate training accuracy</span>
   <span class="n">curr_W</span><span class="p">,</span><span class="n">curr_b</span><span class="p">,</span><span class="n">currr_loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">loss</span><span class="p">],</span><span class="n">feed_dict</span><span class="p">)</span>
   <span class="k">print</span><span class="p">(</span><span class="s">"W: </span><span class="si">%</span><span class="s">s,b: </span><span class="si">%</span><span class="s">s,loss: </span><span class="si">%</span><span class="s">s"</span><span class="o">%</span><span class="p">(</span><span class="n">curr_W</span><span class="p">,</span><span class="n">curr_b</span><span class="p">,</span><span class="n">currr_loss</span><span class="p">))</span>
   <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">]))</span></code></pre></figure>

<p>经过1000次训练，我们可以从下图的运行结果可以发现 loss 越来越小了，说明我们的模型值越来越精准。准确率接近100%<br />
最终我们训练出来的模型 W:2.0000005, b:-1.0000029 loss:1.3187673e-11<br />
从 loss 看，训练出来的模型这已经很精准了。</p>

<p>其实作者给出的值(x_train = [1,2,3,4,5,6], y_train = [1,3,5,7,9,11]<br />
 )的设定函数就是 y=2x-1 (w=2,b=-1)</p>

<p>对我们程序开始给的初始化的值 w=1.1 b=-2.1，是非常不靠谱的，我们看到 loss 尽然是122这么高。通过1000次训练，得到了相当大的矫正。</p>

<p>我想训练出来的模型准确率之所以这么高，一是因为我们给的训练数据比较少，而是因为训练数据给的很理想。<br />
现实生活中的数据可比这复杂多了，当然我们也有更好的模型来训练它，这是后话了。</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">运行结果：
real vale: <span class="o">[</span>-0.9999999   0.10000014  1.2000003   2.3000002   3.4         4.61      <span class="o">]</span>
curr loss: 121.132095337
train step<span class="o">(</span>0<span class="o">)</span> loss:107.545296
train step<span class="o">(</span>100<span class="o">)</span> loss:0.0296359
train step<span class="o">(</span>200<span class="o">)</span> loss:0.01862564
train step<span class="o">(</span>300<span class="o">)</span> loss:0.018511334
train step<span class="o">(</span>400<span class="o">)</span> loss:0.018509919
train step<span class="o">(</span>500<span class="o">)</span> loss:0.018509876
train step<span class="o">(</span>600<span class="o">)</span> loss:0.018509895
train step<span class="o">(</span>700<span class="o">)</span> loss:0.018509895
train step<span class="o">(</span>800<span class="o">)</span> loss:0.018509895
train step<span class="o">(</span>900<span class="o">)</span> loss:0.018509895
W: <span class="o">[</span>1.97131],b: <span class="o">[</span>-0.93244034],loss: 0.018509895
<span class="o">[</span>array<span class="o">([</span>1.97131], <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)</span>, array<span class="o">([</span>-0.93244034], <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)]</span></code></pre></figure>

<p>从 tensorboard 上来看，loss 矫正的速度还是挺快的，大概在第200步之后，模型就稳定了。</p>

<p><img src="/img/post-tf/tf22.png" alt="" /></p>

<p>通过上面的案例，我们再回到文章开篇提出的问题。加入我们知道小明数次选择，假使小明我们的考虑的因素是固定不变的而且外部环境没有发生变化，下次要不要出门，我们就能求出概率啦。哈哈。。。</p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-prev post-nav-item">
    
    <a href="/blog/2018/urlib/"> Python扒取网络图片</a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2018/tensor/"> TensorFlow 学习入门</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2019 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>