---
layout:     post
title:      "文字转语音TTS"
date:       2019-12-13 03:00:00
author:     "Huailiang"
tags:
    - 人工智能
---

##  概述

Tacotron2是由Google Brain 2017年提出来的一个语音合成框架。Tacotron2:一个完整神经网络语音合成方法。模型主要由三部分组成：

• 声谱预测网络：一个引入注意力机制（attention）的基于循环的Seq2seq的特征预测网络，用于从输入的字符序列预测梅尔频谱的帧序列。
• 声码器（vocoder）：一个WaveNet的修订版，用预测的梅尔频谱帧序列来生成时域波形样本。
• 中间连接层：使用低层次的声学表征-梅尔频率声谱图来衔接系统的两个部分。

![](/img/post-ml/tts2.jpg)


## 预处理

### 文字处理

文字主要由字幕和标点、空格等组成， 这里先将文字转换成对应的向量， 如果是汉字的话， 可以先转成拼音。 


1. 先清除文字里陌生字符， 可以使用正则表达式匹配
   
   ```py
   _curly_re = re.compile(r'(.*?)\{(.+?)\}(.*)')
   ```

2. 对每个字母进行编码, 得到一个词向量， 然后传给模型
   ```py
    _pad = '_'
    _punctuation = '!\'(),.:;? '
    _special = '-'
    _letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'

    # Prepend "@" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):
    _arpabet = ['@' + s for s in cmudict.valid_symbols]

    # Export all symbols:
    symbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet

    def sequence_to_text(sequence):
        result = ''
        for symbol_id in sequence:
            if symbol_id in _id_to_symbol:
                s = _id_to_symbol[symbol_id]
                if len(s) > 1 and s[0] == '@':
                    s = '{%s}' % s[1:]
                result += s
        return result.replace('}{', ' ')
   ```


### 声音处理

    一般很难直接能从声音的波形图里，提取出声音的特征，需要先进行转换。 一般的话，都是转换为梅尔图谱或者梅尔频率倒谱系数 MFCC， 在骨骼的论文里转换成了梅尔图谱。

    •  使用librosa或者scipy.io.wavfile 加载到内存， 得到numpy array

    ```py
    from scipy.io.wavfile import read
    sampling_rate, data = read(full_path)
    ```

    •  进行加窗、短时傅里叶变换进入复数域
    
    ```py
    from scipy.signal import get_window

    # get window and zero center pad it to filter_length
    fft_window = get_window(window, win_length, fftbins=True)
    fft_window = pad_center(fft_window, filter_length)
    fft_window = torch.from_numpy(fft_window).float()
    forward_basis *= fft_window
    inverse_basis *= fft_window
    
    forward_transform = F.conv1d(
            input_data,
            Variable(self.forward_basis, requires_grad=False),
            stride=self.hop_length, padding=0)
            
    cutoff = int((self.filter_length / 2) + 1)
    real_part = forward_transform[:, :cutoff, :]  # 实部
    imag_part = forward_transform[:, cutoff:, :]  # 虚部
    magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)
    mel_output = torch.matmul(self.mel_basis, magnitudes)
    ```
   


## 注意力机制

注意力机制是一种在编码器-解码器结构中使用到的机制, 现在已经在多种任务中使用:
• 机器翻译(Neural Machine Translation, NMT)
• 图像描述(Image Captioning (translating an image to a sentence))
• 文本摘要(Summarization(translating to a more compact language))

而且也不再局限于编码器-解码器结构, 多种变体的注意力结构, 应用在各种任务中.

总的来说, 注意力机制应用在:

允许解码器在序列中的多个向量中, 关注它所需要的信息, 是传统的注意力机制的用法. 由于使用了编码器多步输出, 而不是使用对应步的单一定长向量, 因此保留了更多的信息.
作用于编码器, 解决表征问题(例如Encoding Vector再作为其他模型的输入), 一般使用自注意力(self-attention)

### 1. 编码器-解码器注意力机制

###### 1.1 编码器-解码器结构

![](/img/post-ml/tts1.jpg)

如上图, 编码器将输入嵌入为一个向量, 解码器根据这个向量得到输出. 由于这种结构一般的应用场景(机器翻译等), 其输入输出都是序列, 因此也被称为序列到序列的模型Seq2Seq.

对于编码器-解码器结构的训练, 由于这种结构处处可微, 因此模型的参数θ可以通过训练数据和最大似然估计得到最优解, 最大化对数似然函数以获得最优模型的参数, 即:

$$ {\operatorname{arg\,}}\underset{\theta}{\operatorname{\max}}\,  {\sum_{(x,y)}{\log(y|x;\theta)}}  $$

这是一种端到端的训练方法.

###### 1.2 编码器

原输入通过一个网络模型(CNN, RNN, DNN), 编码为一个向量. 由于这里研究的是注意力, 就以双向RNN作为示例模型.

对于每个时间步t, 双向RNN编码得到的向量ht可以如下表示:

$$  h_t = \left[ \overrightarrow{h_t}; \overleftarrow{h_t} \right]  $$


###### 1.3 解码器

这里的解码器是单向RNN结构, 以便在每个时间点上产生输出, 行程序列. 由于解码器仅使用最后时间步T对应的编码器的隐藏向量hTx, Tx指的是当前样本的时间步长度(对于NLP问题, 经常将所有样本处理成等长的). 这就迫使编码器将更多的信息整合到最后的隐藏向量hTx中.

但由于hTx是单个长度一定的向量, 这个单一向量的表征能力有限, 包含的信息量有限, 很多信息都会损失掉.

注意力机制允许解码器在每一个时间步t处考虑整个编码器输出的隐藏状态序列(h1,h2,⋯,hTx), 从而编码器将更多的信息分散地保存在所有隐藏状态向量中, 而解码器在使用这些隐藏向量时, 就能决定对哪些向量更关心.

具体来说, 解码器生产的目标序列(y1,⋯,yTx)中的每一个输出(如单词)yt, 都是基于如下的条件分布:


$$  P[y_t|\{y1,···, y_{t-1}\}, c_t] = softmax(W_s\overline{h_t})   $$

其中$\overline{h_t}$是引入注意力的隐藏状态向量(attentional hidden state), 如下得到:

$$   \overline{h_t} = tanh(W_c[c_t;h_t])    $$

$h_t$为编码器顶层的隐藏状态, $c_t$是上下文向量, 是通过当前时间步上下文的隐藏向量计算得到的, 主要有全局和局部两种计算方法, 下午中提到. Wc和Ws参数矩阵训练得到. 为了式子的简化没有展示偏置项.

###### 1.4 全局注意力

通过全局注意力计算上下文向量ct时, 使用整个序列的隐藏向量ht, 通过加权和的方式获得. 假设对于样本x的序列长度为Tx, ct如下计算得到:

$$    c_t = \sum_{i=1}^{T_X}{\alpha_{t,i}h_i}       $$


其中长度为$T_x$的校准向量alignment vector αt的作用是在t时间步, 隐藏状态序列中的所有向量的重要程度. 其中每个元素$\alpha_{t,i}$的使用softmax方法计算:


$$ \alpha_{t,i} = \frac{exp(score(h_t,h_i))}{\sum_{j=1}^{T_x}{exp(score(h_t,h_j))}}   $$


score函数可以是任意的比对向量的函数, 一般常用:

点积: $ score(h_t,h_i)=h_t^T h_i $

这在使用全局注意力时有更好的效果

使用参数矩阵:

$ score(h_t,h_i)=h_t^tW_αh_i $

这就相当于使用了一个全连接层, 这种方法在使用局部注意力时有更好的效果.

![](/img/post-ml/tts3.jpg)


###### 1.5 局部注意力

全局注意力需要在序列中所有的时间步上进行计算, 计算的代价是比较高的, 可以使用固定窗口大小的局部注意力机制, 窗口的大小为2D+1. D为超参数, 为窗口边缘距离中心的单方向距离. 上下文向量ct的计算方法如下:

$$     c_t = \sum_{i=p_t-D}^{p_t+D}{\alpha_{t,i}h_i}      $$


可以看到, 只是考虑的时间步范围的区别, 其他完全相同. pt作为窗口的中心, 可以直接使其等于当前时间步t, 也可以设置为一个变量, 通过训练获得, 即:


$$  p_t = T_x\sigma(\nu_p^T \tanh(W,_PH_T)) $$


其中σ为sigmoid函数, $\nu_p$和 $W_p$均为可训练参数. 因此这样计算得到的pt是一个浮点数, 但这并没有影响, 因为计算校准权重向量$α_t$时, 增加了一个均值为pt, 标准差为D2的正态分布项:

$$ \alpha_{t,i} = \frac{exp(score(h_t, h_i))}{\sum_{j=1}^{T_x}{exp(score(h_t,h_j))}} exp(-\frac{(i-p_t)^2}{2(D/2)^2}) $$

局部注意力机制总结如下图:

![](/img/post-ml/tts4.jpg)