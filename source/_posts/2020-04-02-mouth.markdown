---
layout:     post
title:      "基于深度学习的口型动画合成系统"
date:       2020-04-02 02:00:00
author:     "huailiang"
tags:
    - 人工智能
---


>转发一篇公司内部分享的关于语音嘴型的文字


## 前言

近年来，随着深度学习为代表的AI浪潮的到来，AI这项高大上的技术也逐渐触手可及，有些应用甚至引爆了公众。想象一下，当你跟游戏中的NPC或动画中的虚拟角色交互对话时，NPC开口说话了，而且嘴型随声而动，这个要求一点也不高吧。但殊不知，这个嘴型动画与语音相匹配的事情，其实并不好做，甚至需要美术同学大量的手工K帧。现在，有了深度学习的方案，这都不是事啦！那么这个系统到底是怎么工作的呢？且听我慢慢道来。

<video id="video" controls="" preload="none" poster="/img/post-reinforcement/re6.jpg" width="674" height="379">
      <source id="mp4" src="/img/post-vr/mou1.mp4" type="video/mp4">
      <p>Your user agent does not support the HTML5 Video element.</p>
</video>

<video id="video" controls="" preload="none" poster="/img/post-reinforcement/re6.jpg" width="674" height="379">
      <source id="mp4" src="/img/post-vr/obama.mp4" type="video/mp4">
      <p>Your user agent does not support the HTML5 Video element.</p>
</video>

## 原理

我们的想法很直接，就是基于音频生成口型动画，或者说，输入是一段音频，经过我们的系统，输出为相应的口型动画。而如果先忽略音频和动画的时序，问题就变成了将音频关键帧经过一个深度神经网络，得到一个口型关键帧。那么就存在两个问题：

* 1、如何表示音频关键帧和口型关键帧？
* 2、网络如何设计？

### 特征表示

__音频特征__

对于问题1，我们先说音频特征。这里我们采用的音频特征是语音识别领域内常用的梅尔频率倒谱系数（MFCCs）。梅尔频率倒谱系数是受人的听觉系统研究成果推动而导出的声学特征，它对于声音信号处理更接近人耳对声音的分析特性，能够准确的描述语音短时功率谱的包络，从而很好的反应出声道形状。


![](/img/post-vr/mou2.jpg)


在特征提取过程中，我们先以20ms的帧长和10ms的帧移对音频进行分帧处理，并计算每个音频帧的梅尔频率倒谱系数（系数个数为M=13）。计算完音频帧的梅尔频率倒谱系数后，我们还获取了它的一阶差分系数和二阶差分系数。该差分系数用来描述动态特征，即声学特征在相邻特征间的变化情况。

经过上述处理之后，针对每个音频帧，我们都有一个13x3维的梅尔频率倒谱特征，用于描述当前音频帧的包络和声学特征的变化信息。理论上，我们可以直接用它来表示口型帧对应的音频帧，但为了更准确的捕获音频的上下文信息，我们在实际处理时会以口型帧对应的音频帧为中心，选取前后共N=16帧的音频帧的梅尔频率倒谱特征作为当前音频帧的特征。这样，我们最终的音频特征就是一个16x13x3维的特征了。


__口型特征__

有了音频特征，接下来就是口型特征了。这里，我们参考  提取出40个通用音素的权重作为口型帧的表示。在具体实现时，我们按发音口型将40个音素分为了11个音素组，并针对每个音素组制作其相应的嘴型。值得一提的是，音素组数目的选取和嘴型的制作方法（基于骨骼动画或者基于BlendShape）可由使用者自行选定，这里并不会影响底层算法的实现

![](/img/post-vr/mou3.jpg)

## 网络架构

该如何搭建网络呢？这里我们参考了Karras等人在SIGGRAPH 2017的论文 [Audio driven Facial Animation by Joint End-to-end Learning of Pose and Emotion][i1] 中的工作，搭建了图 1‑5中所示的网络架构。


![](/img/post-vr/mou4.jpg)

该架构由输入层、谱分析网络、协同发音网络和输出层等四部分组成。输入层接受音频特征，并通过不含激活函数的卷积层做初始变换。然后谱分析网络在谱特征维度上对特征进行分析。紧接着，协同发音网络在时域上对提取的特征做进一步分析。最后，输出层通过1x1的卷积核将特征映射成口型特征。网络的详细配置见表。


![](/img/post-vr/mou5.jpg)

## 数据集

实验中，我们采用了LibriSpeech和AISHELL两个大型语料数据集，其中英文音频要多于中文音频，这也使得系统对于英文的口型合成的效果要略好于中文。两个数据集上，音频特征的提取是通过我们编写的代码得到的，而口型特征的提取则分为两个部分：针对英文音频，40个音素权重是利用第三方SDK Annosoft生成的。针对中文音频，这40个音素权重是本组基于Annosoft进行改进优化之后重新生成的。

那我们为什么不直接使用第三方SDK制作口型动画呢？主要原因有两点：

1. 这两种方法均需要字幕文件（Annosoft提供不带字幕的音素标注方法，但效果很差）。
2. 这两种方法针对不同语种音频的音素标注需要的配置文件不相同，操作比较繁琐。而我们的系统针对中英文音频的音素标注可使用同一套配置，且仅需音频文件就可合成高质量的嘴型动画。


## 总结

我们实现了一套口型动画合成系统，该系统利用深度学习完成从语音到口型动画的映射，可以有效解决语音动画同步的难题，增强动画的真实感和逼真性。同时，该系统对于说话人和语言不敏感，对于中英文的支持普遍好于市面上的同类产品。此外，该系统由于只需要音频文件，所以极大的简化了口型动画的制作流程，减少了相关的时间成本和人员开销。

当然，该系统还存在一定的局限性，具体表现为两方面：

1. 该系统没有对人说话时的情绪和说话风格做特殊处理，这导致带情绪和不带情绪说话时合成的口型动画区别不明显，其主要原因是样本数据中带情绪的音频过少，难以提取出情绪特征。
2.  该系统暂时无法根据音频内容生成相应的表情动画，这主要是由于相比于口型动画，表情动画的制作会简单得多，也就没有成为我们的研究重点。需要说明的是，我们的系统支持二次编辑，允许用户在口型动画的基础上添加表情。

最后，该系统现已集成到Unity插件中，并被用于主机项目中。我们可以提供全套的口型动画生成支持或者根据现有的口型制作流程调整适配我们的口型动画生成系统，感兴趣的同学欢迎联系我们，一起交流，共同进步。


[i1]: https://research.nvidia.com/publication/2017-07_Audio-Driven-Facial-Animation