---
layout:     post
title:      "移动端AI算计加速"
date:       2021-05-26 02:00:00
author:     "huailiang"
tags:
    - 人工智能
---

> 目前支持移动端（手机端）的框架， 主流的实现架构诸如苹果公司的CoreML, 华为麒麟芯片的支持AI加速的NPU和在上面运行框架HiAI，阿里的优化框架MNN， 还有腾讯公司的NCNN。 过去我们使用的AI运算大都是PC端搭建的GPU集群，往往运行着后端（云侧）， 而在端侧往往只是负责拿到云侧数据进行表现。 随着手机端性能的提升， 其巨大的算力给AI拓扑带来了无限的可能。

#### 一、概述

目前AI的主流实现都是基于反向传播算法的神经网络，前向传播进行推理， 反向传播计算梯度来更新参数。 主机上运行的主流框架多是谷歌公司的Tensorflow (Lite), Facebook公司开发的Pytorch 以及 Caffe, 然后他们在设计这些框架的时候， 往往更多的考虑的是算法的覆盖， 而不是性能。 我们常常看到这些复杂的网络结构大多跑在Nvidia的高端显卡上， 训练时间往往短则数日， 长则数周以致多大一两个月， 比如说Nvidia实现的StyleGAN。 由于这些特性， 特别不适应手机端的芯片。

![](/img/post-ml/mnn.jpeg)

随着国内厂商不断的优化， 我们看到了华为推出的麒麟芯片（麒麟980之后带独立的NPU）使用[HiAI][i2]框架来进行AI加速运算， [阿里的MNN][i1]则依靠大量手写汇编实现核心运算，充分发挥 ARM CPU 的算力， 整合不同后端（backend: OpenCL、Vulkan、OpenGL, Metal)进行深度优化来适配不同的设备。这些国内的厂商都提供了各自的工具（比如说华为HiAI的转换工具OMG）来转换主机上运行的Tensorflow、Caffee模型为自己量化的模型。

![](/img/post-ml/hiai.jpeg)

#### 二、环境


我们看到国内厂商推出的Demo都是基于原生的语言开发出来的应用， 目前对游戏引擎这种跨平台的应用支持的还不多，这里大多是由于移动端的算子支持还不够全面， 从而导致有些主机上的模型导致转换失败，这就限制了移动端从主机端迁移的速度， 不过类似经典的MobileNet_v2都是支持的。


[i1]: https://github.com/alibaba/MNN
[i2]: https://developer.huawei.com/consumer/cn/hiai#Foundation